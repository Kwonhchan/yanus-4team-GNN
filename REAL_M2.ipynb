{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN, GAT, GraphSAGE, Unet구조, skip-connection, residual Connections, 멀티-헤드 Attention 메커니즘\n",
    "class m2_model(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(m2_model, self).__init__()\n",
    "        # Down-sampling Path 제거됨\n",
    "        self.sage1 = SAGEConv(num_node_features, 64)\n",
    "        self.sage2 = SAGEConv(64, 128)\n",
    "\n",
    "        # Bottleneck with multi-head attention\n",
    "        self.gat = GATConv(128, 128, heads=8, concat=True)\n",
    "\n",
    "        # Up-sampling Path\n",
    "        self.gcn1 = GCNConv(128 * 8, 64)  # Adjusted for concatenated multi-head attention output\n",
    "        self.gcn2 = GCNConv(64, num_classes)\n",
    "\n",
    "        # Residual Connections and Dimension Matching\n",
    "        self.res1 = torch.nn.Linear(num_node_features, 64)\n",
    "        self.res2 = torch.nn.Linear(64, 128 * 8)  # Adjust for concatenated multi-head attention output\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # Initial residual connections\n",
    "        res_x = self.res1(x)\n",
    "\n",
    "        # Contracting path without TopKPooling\n",
    "        x = F.relu(self.sage1(x, edge_index)) + res_x\n",
    "        x1 = x  # Skip connection\n",
    "\n",
    "        x = F.relu(self.sage2(x, edge_index))\n",
    "\n",
    "        # Bottleneck with GAT for attention mechanism\n",
    "        x = F.relu(self.gat(x, edge_index))\n",
    "\n",
    "        # Up-sampling path with GCN for refining features\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_and_splitter():\n",
    "    with open('custom_dataset.pkl', 'rb') as f:\n",
    "        custom_dataset = pickle.load(f)\n",
    "    with open('data_splitter.pkl', 'rb') as f:\n",
    "        data_splitter = pickle.load(f)\n",
    "    return custom_dataset, data_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset and splitter from saved files.\n"
     ]
    }
   ],
   "source": [
    "# 저장된 객체를 불러옵니다.\n",
    "custom_dataset, data_splitter = load_dataset_and_splitter()\n",
    "print(\"Loaded dataset and splitter from saved files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, base_loss_function=nn.CrossEntropyLoss()):\n",
    "        super().__init__()\n",
    "        self.base_loss_function = base_loss_function\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # -1 레이블을 가진 타겟은 손실 계산에서 제외\n",
    "        valid_indices = targets != -1\n",
    "        if valid_indices.any():\n",
    "            return self.base_loss_function(predictions[valid_indices], targets[valid_indices])\n",
    "        else:\n",
    "            return torch.tensor(0.0).to(predictions.device)  # 모든 타겟이 -1인 경우 0 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    valid_indices = target != -1\n",
    "    if valid_indices.any():\n",
    "        preds = output[valid_indices].argmax(dim=1)\n",
    "        correct = (preds == target[valid_indices]).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model = m2_model(num_node_features=4, num_classes=41476).to(device)\n",
    "custom_loss_function = CustomLoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = CustomLoss()\n",
    "\n",
    "# 데이터 로더 생성 및 데이터 분할\n",
    "train_loader, val_loader, test_loader = data_splitter.split_data()\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/2484 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 2484/2484 [00:28<00:00, 87.75it/s, loss=10.4, train_acc=0.403] \n",
      "Validating: 100%|██████████| 710/710 [00:04<00:00, 143.61it/s, val_acc=0.38, val_loss=10.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 2484/2484 [00:25<00:00, 96.13it/s, loss=9.59, train_acc=0.352] \n",
      "Validating: 100%|██████████| 710/710 [00:04<00:00, 147.56it/s, val_acc=0.38, val_loss=11.4] \n",
      "Epoch 3/100: 100%|██████████| 2484/2484 [00:26<00:00, 95.47it/s, loss=8.89, train_acc=0.354] \n",
      "Validating: 100%|██████████| 710/710 [00:04<00:00, 151.09it/s, val_acc=0.17, val_loss=12.6] \n",
      "Epoch 4/100: 100%|██████████| 2484/2484 [00:25<00:00, 96.69it/s, loss=8.63, train_acc=0.313]\n",
      "Validating: 100%|██████████| 710/710 [00:04<00:00, 145.46it/s, val_acc=0.0999, val_loss=13.7]\n",
      "Epoch 5/100: 100%|██████████| 2484/2484 [00:25<00:00, 97.35it/s, loss=8.44, train_acc=0.293] \n",
      "Validating: 100%|██████████| 710/710 [00:04<00:00, 144.33it/s, val_acc=0.114, val_loss=15]  \n",
      "Epoch 6/100:  28%|██▊       | 687/2484 [00:07<00:19, 93.86it/s, loss=8.29, train_acc=0.347] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kwonh\\anaconda3\\envs\\AllLeave\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TensorBoard 요약 작성자 설정\n",
    "writer = SummaryWriter('runs/experiment_name')\n",
    "\n",
    "def save_checkpoint(state, filename=\"best_check/my_checkpoint_2rd.pth\"):\n",
    "        print(\"=> Saving checkpoint\")\n",
    "        torch.save(state, filename)\n",
    "\n",
    "best_val_acc = 0.0  # 가장 높은 검증 정확도 저장을 위한 변수\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        progress_bar = tqdm(iter(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch_idx, data in enumerate(progress_bar):\n",
    "                data = data.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, data.y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_acc = accuracy(outputs, data.y)  # 훈련 정확도 계산\n",
    "                train_correct += (train_acc * data.y.size(0)).item()\n",
    "                train_total += data.y.size(0)\n",
    "\n",
    "                progress_bar.set_postfix(loss=running_loss/(batch_idx+1), train_acc=100. * train_correct / train_total)\n",
    "\n",
    "        # 훈련 손실 및 정확도 로깅\n",
    "        writer.add_scalar('training loss', running_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('training accuracy', 100. * train_correct / train_total, epoch)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad(), tqdm(val_loader, desc='Validating') as progress_bar:\n",
    "                for data in progress_bar:\n",
    "                        data = data.to(device)\n",
    "                        outputs = model(data)\n",
    "                        loss = criterion(outputs, data.y)\n",
    "\n",
    "                        val_loss += loss.item()\n",
    "                        val_acc = accuracy(outputs, data.y)  # 검증 정확도 계산\n",
    "                        val_correct += (val_acc * data.y.size(0)).item()\n",
    "                        val_total += data.y.size(0)\n",
    "\n",
    "                        progress_bar.set_postfix(val_loss=val_loss/len(val_loader), val_acc=100. * val_correct / val_total)\n",
    "\n",
    "        # 검증 손실 및 정확도 로깅\n",
    "        writer.add_scalar('validation loss', val_loss / len(val_loader), epoch)\n",
    "        writer.add_scalar('validation accuracy', 100. * val_correct / val_total, epoch)\n",
    "\n",
    "        # 체크포인트 저장 조건\n",
    "        if 100. * val_correct / val_total > best_val_acc:\n",
    "                best_val_acc = 100. * val_correct / val_total\n",
    "                checkpoint_filename = f\"best_check/checkpoint_epoch_{epoch+1}_4rd.pth\"\n",
    "                save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_loss,\n",
    "                'accuracy': best_val_acc,\n",
    "                }, filename=checkpoint_filename)\n",
    "\n",
    "writer.close()  # TensorBoard 작성자 닫기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedLabelEncoder1:\n",
    "    def __init__(self, base_encoder):\n",
    "        # base_encoder는 LabelEncoder의 인스턴스입니다.\n",
    "        self.base_encoder = base_encoder\n",
    "    \n",
    "    def fit(self, y):\n",
    "        self.base_encoder.fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y, unknown_label=-1):\n",
    "        new_y = []\n",
    "        for item in y:\n",
    "            if item in self.base_encoder.classes_:\n",
    "                # 기존 base_encoder의 transform 메서드를 사용하여 변환합니다.\n",
    "                encoded = self.base_encoder.transform([item])[0]\n",
    "                new_y.append(encoded)\n",
    "            else:\n",
    "                # 새로운 항목은 unknown_label 값으로 처리\n",
    "                new_y.append(unknown_label)\n",
    "        return np.array(new_y)\n",
    "    \n",
    "class ExtendedLabelEncoder2:\n",
    "    def __init__(self, base_encoder):\n",
    "        self.base_encoder = base_encoder  # 기존 LabelEncoder 인스턴스\n",
    "        self.unknown_label_start = -1\n",
    "        self.unknown_labels_dict = {}\n",
    "    \n",
    "    def fit(self, y):\n",
    "        self.base_encoder.fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y, unknown_label_start=-1):\n",
    "        new_y = []\n",
    "        for item in y:\n",
    "            try:\n",
    "                # base_encoder의 classes_를 참조하여 transform을 시도합니다.\n",
    "                if item in self.base_encoder.classes_:\n",
    "                    encoded = self.base_encoder.transform([item])[0]\n",
    "                    new_y.append(encoded)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            except ValueError:\n",
    "                # 새로운 아이템 처리\n",
    "                if item not in self.unknown_labels_dict:\n",
    "                    self.unknown_labels_dict[item] = self.unknown_label_start\n",
    "                    self.unknown_label_start -= 1\n",
    "                new_y.append(self.unknown_labels_dict[item])\n",
    "        return np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일로부터 CustomDataset 객체 로드\n",
    "with open('custom_dataset.pkl', 'rb') as f:\n",
    "    loaded_custom_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataU:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.user_encoder = ExtendedLabelEncoder1(loaded_custom_dataset.user_encoder)\n",
    "        self.item_encoder = ExtendedLabelEncoder2(loaded_custom_dataset.item_encoder)\n",
    "        self.gender_encoder = ExtendedLabelEncoder1(loaded_custom_dataset.gender_encoder)\n",
    "        self.prepare_data()\n",
    "        self.graphs = []\n",
    "        self.create_individual_graphs()\n",
    "        self.pyg_graphs = []\n",
    "        self.create_pyg_list()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.df['user_index'] = self.user_encoder.transform(self.df['TRAVEL_ID'],unknown_label=-1)\n",
    "        self.df['item_index'] = self.item_encoder.transform(self.df['VISIT_AREA_NM'])\n",
    "        self.df['GENDER_index'] = self.gender_encoder.transform(self.df['GENDER'],unknown_label=-1)\n",
    "\n",
    "    def create_individual_graphs(self):\n",
    "        for _, group in self.df.groupby('TRAVEL_ID'):\n",
    "            G = nx.Graph()\n",
    "            user_index = group['user_index'].iloc[0]\n",
    "            user_attributes = group.iloc[0][['GENDER_index', 'AGE_GRP', 'FAMILY_MEMB', 'TRAVEL_COMPANIONS_NUM']].to_dict()\n",
    "            G.add_node(user_index, **user_attributes, type='user')\n",
    "            for _, row in group.iterrows():\n",
    "                item_index = row['item_index']\n",
    "                G.add_node(item_index, type='item', name=row['VISIT_AREA_NM'])\n",
    "                edge_attributes = row[['RESIDENCE_TIME_MIN', 'DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']].to_dict()\n",
    "                G.add_edge(user_index, item_index, **edge_attributes)\n",
    "            self.graphs.append(G)\n",
    "\n",
    "    def graph_to_pygdata(self, G):\n",
    "        node_features, node_labels, edge_index, edge_attr = [], [], [], []\n",
    "        node_index_mapping = {node: i for i, (node, _) in enumerate(G.nodes(data=True))}\n",
    "        for node, attr in G.nodes(data=True):\n",
    "            if 'type' in attr and attr['type'] == 'user':\n",
    "                node_features.append([attr['GENDER_index'], attr['AGE_GRP'], attr['FAMILY_MEMB'], attr['TRAVEL_COMPANIONS_NUM']])\n",
    "                node_labels.append(-1)\n",
    "            else:\n",
    "                node_features.append([0, 0, 0, 0])\n",
    "                node_labels.append(self.item_encoder.transform([attr['name']])[0])\n",
    "        for source, target, attr in G.edges(data=True):\n",
    "            edge_index.append([node_index_mapping[source], node_index_mapping[target]])\n",
    "            edge_attr.append([attr['RESIDENCE_TIME_MIN'], attr['DGSTFN'], attr['REVISIT_INTENTION'], attr['RCMDTN_INTENTION']])\n",
    "        data = Data(x=torch.tensor(node_features, dtype=torch.float),\n",
    "                    edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous(),\n",
    "                    edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "                    y=torch.tensor(node_labels, dtype=torch.long))\n",
    "        return data\n",
    "\n",
    "    def create_pyg_list(self):\n",
    "        for G in self.graphs:\n",
    "            self.pyg_graphs.append(self.graph_to_pygdata(G))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환완료\n",
      "모델 돌아가는 중\n",
      "결과\n",
      "All recommended items for the user: ['서귀포매일올레시장' '서귀포매일올레시장' '서귀포매일올레시장']\n"
     ]
    }
   ],
   "source": [
    "# 웹에서 받은 데이터 예제\n",
    "raw_data = \"\"\"TRAVEL_ID,VISIT_AREA_NM,RESIDENCE_TIME_MIN,DGSTFN,REVISIT_INTENTION,RCMDTN_INTENTION,GENDER,AGE_GRP,FAMILY_MEMB,TRAVEL_COMPANIONS_NUM\n",
    "d_d000249,디앤디파트먼트 제주,60.0,5.0,5.0,5.0,남,30,1,3\n",
    "d_d000249,제주동문시장,30.0,5.0,5.0,5.0,남,30,1,3\n",
    "\"\"\"\n",
    "df1 = pd.read_csv(io.StringIO(raw_data))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = r\"best_check\\checkpoint_epoch_1_3rd.pth\"  # 경로 구분자 수정\n",
    "\n",
    "# 체크포인트 로드 및 모델 상태 사전 추출\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model = m2_model(num_node_features=4, num_classes=41476).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "graph_data = GraphDataU(df1)\n",
    "\n",
    "\n",
    "# loaded_custom_dataset에서 GraphData 속성을 추출\n",
    "print(\"변환완료\")\n",
    "recommendations = []\n",
    "\n",
    "data = graph_data.pyg_graphs[0].to(device)\n",
    "with torch.no_grad():\n",
    "    print(\"모델 돌아가는 중\")\n",
    "    output = model(data)\n",
    "    print(\"결과\")\n",
    "\n",
    "predicted_item_index = output.argmax(dim=1).cpu().numpy()\n",
    "predicted_item_names = loaded_custom_dataset.item_encoder.inverse_transform(predicted_item_index)\n",
    "\n",
    "# 모든 추천 항목 출력 (중복 포함)\n",
    "print(f\"All recommended items for the user: {predicted_item_names}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AllLeave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
